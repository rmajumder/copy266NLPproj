{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import original data\n",
    "df_orginal = pd.read_excel('processed/combine_aspect_position.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17126, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orginal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Review ID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Version</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>Doc Sentiment</th>\n",
       "      <th>Asp Sentiment</th>\n",
       "      <th>Review</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>AspectText</th>\n",
       "      <th>Positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5464235433</td>\n",
       "      <td>Canada</td>\n",
       "      <td>20.01.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>latest app update. this update is terrible bas...</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>7,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5464235433</td>\n",
       "      <td>Canada</td>\n",
       "      <td>20.01.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>latest app update. this update is terrible bas...</td>\n",
       "      <td>updates</td>\n",
       "      <td>update</td>\n",
       "      <td>11,16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5466985498</td>\n",
       "      <td>USA</td>\n",
       "      <td>20.01.5</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>life saver. one my favorite apps to use while ...</td>\n",
       "      <td>app</td>\n",
       "      <td>apps</td>\n",
       "      <td>28,31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5460552711</td>\n",
       "      <td>USA</td>\n",
       "      <td>20.01.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst update yet. just deleted. crashed. didn’...</td>\n",
       "      <td>updates</td>\n",
       "      <td>update</td>\n",
       "      <td>6,11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5460552711</td>\n",
       "      <td>USA</td>\n",
       "      <td>20.01.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst update yet. just deleted. crashed. didn’...</td>\n",
       "      <td>experience</td>\n",
       "      <td>deleted</td>\n",
       "      <td>23,29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1   Review ID Country  Version  Rating       Date  \\\n",
       "0           0             0  5464235433  Canada  20.01.5       1 2020-01-30   \n",
       "1           1             1  5464235433  Canada  20.01.5       1 2020-01-30   \n",
       "2           2             2  5466985498     USA  20.01.5       5 2020-01-30   \n",
       "3           3             3  5460552711     USA  20.01.4       1 2020-01-29   \n",
       "4           4             4  5460552711     USA  20.01.4       1 2020-01-29   \n",
       "\n",
       "  Doc Sentiment Asp Sentiment  \\\n",
       "0      negative       neutral   \n",
       "1      negative      negative   \n",
       "2      positive      positive   \n",
       "3      negative      negative   \n",
       "4      negative      negative   \n",
       "\n",
       "                                              Review     Aspects AspectText  \\\n",
       "0  latest app update. this update is terrible bas...         app        app   \n",
       "1  latest app update. this update is terrible bas...     updates     update   \n",
       "2  life saver. one my favorite apps to use while ...         app       apps   \n",
       "3  worst update yet. just deleted. crashed. didn’...     updates     update   \n",
       "4  worst update yet. just deleted. crashed. didn’...  experience    deleted   \n",
       "\n",
       "  Positions  \n",
       "0       7,9  \n",
       "1     11,16  \n",
       "2     28,31  \n",
       "3      6,11  \n",
       "4     23,29  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orginal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove HTML, numbers, Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeHtml(text):\n",
    "    cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eelrufaie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eelrufaie/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_orginal.iterrows():\n",
    "    text = df_orginal.loc[index, 'Review']\n",
    "    cleaned_text = removeHtml(text)   \n",
    "    lemmatized_word = lemmatize_text(cleaned_text)\n",
    "    \n",
    "    df_orginal.loc[index,'Review'] = \" \".join(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [l, e,  , p, p,  , u, p, e, .,  , h,  , u, p, ...\n",
       "1        [l, e,  , p, p,  , u, p, e, .,  , h,  , u, p, ...\n",
       "2        [l, f, e,  , v, e, r, .,  , n, e,  ,  , f, v, ...\n",
       "3        [w, r,  , u, p, e,  , e, .,  , j, u,  , e, l, ...\n",
       "4        [w, r,  , u, p, e,  , e, .,  , j, u,  , e, l, ...\n",
       "                               ...                        \n",
       "17121    [.,  , h, e,  , w, r, !, !, !, !,  ,  , w, c, ...\n",
       "17122    [.,  , c, n, ',  , e,  , n, v, c, e,  , #,  , ...\n",
       "17123    [.,  , n, n,  , f, u, n, c, n,  , c, n,  , g, ...\n",
       "17124    [.,  , c, r, e, n, g,  , f, u, l, l,  , n, v, ...\n",
       "17125    [.,  , w, e, e, .,  , v, e, r,  , e,  ,  , n, ...\n",
       "Name: Review, Length: 17126, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orginal['Review'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [., awesome, ., very, easy, to, navigate, ., a...\n",
       "1        [., awesome, ., very, easy, to, navigate, ., a...\n",
       "2        [., awesome, ., very, easy, to, navigate, ., a...\n",
       "3        [., awesome, ., very, easy, to, navigate, ., a...\n",
       "4        [., awesome, ., very, easy, to, navigate, ., a...\n",
       "                               ...                        \n",
       "17121    [., awesome, ., very, easy, to, navigate, ., a...\n",
       "17122    [., awesome, ., very, easy, to, navigate, ., a...\n",
       "17123    [., awesome, ., very, easy, to, navigate, ., a...\n",
       "17124    [., awesome, ., very, easy, to, navigate, ., a...\n",
       "17125    [., awesome, ., very, easy, to, navigate, ., a...\n",
       "Name: Review, Length: 17126, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "df_orginal['Review'].apply(lambda x: [spell(w) for w in (nltk.word_tokenize(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orginal.to_csv(r'processed/combine_aspect_position_0408_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
