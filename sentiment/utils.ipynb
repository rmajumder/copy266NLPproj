{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference - https://github.com/NLPWM-WHU/TransCap/tree/master/TransCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from afinn import Afinn\n",
    "af = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_position(sptoks, position):\n",
    "    from_idx = int(position.split(',')[0])\n",
    "    to_idx = int(position.split(',')[1])\n",
    "    if from_idx == to_idx == 0:\n",
    "        pos_info = [0] * len(sptoks)\n",
    "    else:\n",
    "        aspect_is = []\n",
    "        \n",
    "        for sptok in sptoks:\n",
    "            if sptok.idx < to_idx and sptok.idx + len(sptok.text) > from_idx:\n",
    "                aspect_is.append(sptok.i)\n",
    "        \n",
    "        #If the aspect position is not found in the tokens\n",
    "        if len(aspect_is) == 0:\n",
    "            return None\n",
    "        \n",
    "        pos_info = []\n",
    "        \n",
    "        #Take the aspect with earliest positioning\n",
    "        for _i, sptok in enumerate(sptoks):\n",
    "            pos_info.append(min([abs(_i - i) for i in aspect_is]) + 1)\n",
    "\n",
    "    return pos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_with_weights_on_polarized_words(sptoks, position):\n",
    "    from_idx = int(position.split(',')[0])\n",
    "    to_idx = int(position.split(',')[1])\n",
    "    if from_idx == to_idx == 0:\n",
    "        pos_info = [0] * len(sptoks)\n",
    "    else:\n",
    "        aspect_is = []\n",
    "        \n",
    "        for sptok in sptoks:\n",
    "            if sptok.idx < to_idx and sptok.idx + len(sptok.text) > from_idx:\n",
    "                aspect_is.append(sptok.i)\n",
    "        \n",
    "        #If the aspect position is not found in the tokens\n",
    "        if len(aspect_is) == 0:\n",
    "            return None\n",
    "        \n",
    "        pos_info = []\n",
    "        \n",
    "        #Take the aspect with earliest positioning\n",
    "        for _i, sptok in enumerate(sptoks):\n",
    "            pos_info.append(min([abs(_i - i) for i in aspect_is]) + 1)\n",
    "\n",
    "        for _i, sptok in enumerate(sptoks):\n",
    "            if pos_info[_i] != 1:\n",
    "                sent_score = abs(af.score(str(sptok)))\n",
    "                if sent_score != 0.0:\n",
    "                    pos_info[_i] = (pos_info[_i]/sent_score) / 200\n",
    "                    #pos_info[_i] = sent_score / pos_info[_i]\n",
    "                #else:\n",
    "                #    pos_info[_i] = 0.1\n",
    "        \n",
    "            \n",
    "    return pos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_label(label):\n",
    "    lab = None\n",
    "    if label == 'negative':\n",
    "        lab = [1, 0, 0]\n",
    "    elif label == 'neutral':\n",
    "        lab = [0, 1, 0]\n",
    "    elif label == \"positive\":\n",
    "        lab = [0, 0, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown label: %s\" % lab)\n",
    "\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'dev/review.txt', \n",
    "#'twt/candidate/2012/romney/review.txt',\n",
    "#                    'twt/candidate/2016/trump/review.txt',\n",
    "#                    'twt/candidate/2016/hillary/review.txt',\n",
    "#                    'twt/candidate/2020/trump/review.txt',\n",
    "#                    'twt/candidate/2020/biden/review.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the input files (train, test, dev, smalltest) and create word to id vector\n",
    "def data_init(path, DSC):\n",
    "    source_count = []\n",
    "    source_word2idx = {}\n",
    "    max_sent_len = 0\n",
    "    for process in ['train/review.txt', 'train/{}_review.txt'.format(DSC), \n",
    "                    'dev/review.txt', 'test/review.txt', \n",
    "                    'twt/candidate/2012/obama/review.txt'\n",
    "                    ]:\n",
    "        print('Processing {}...'.format(process))\n",
    "        fname = path + process        \n",
    "        \n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            source_words = []\n",
    "            for line in lines:\n",
    "                sptoks = en_nlp(line.strip())                                \n",
    "                source_words.extend([sp.text.lower() for sp in sptoks])\n",
    "                if len(sptoks) > max_sent_len:\n",
    "                    max_sent_len = len(sptoks)\n",
    "\n",
    "        if len(source_count) == 0:\n",
    "            source_count.append(['<pad>', 0])\n",
    "        source_count.extend(Counter(source_words).most_common())\n",
    "        for word, _ in source_count:\n",
    "            if word not in source_word2idx:\n",
    "                source_word2idx[word] = len(source_word2idx)\n",
    "\n",
    "    print('max_sentence_length', max_sent_len)\n",
    "    \n",
    "    with open(path+DSC+'_word2id.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(str(source_word2idx))\n",
    "\n",
    "    return source_word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read raw data and create relative position, sentiment and mask vectors\n",
    "def read_data(fname, source_word2idx, max_sent_length, target_maxlen, mode=None):\n",
    "    source_data, target_data, target_label = list(), list(), list()\n",
    "    source_loc = list()\n",
    "    target_mask = list()\n",
    "    target_mode = list()\n",
    "\n",
    "    review = open(fname + r'review.txt', 'r', encoding='utf-8').readlines()\n",
    "    label = open(fname + r'label.txt', 'r', encoding='utf-8').readlines()\n",
    "    term = open(fname + r'term.txt', 'r', encoding='utf-8').readlines()\n",
    "    position = open(fname + r'position.txt', 'r', encoding='utf-8').readlines()\n",
    "    \n",
    "    for index, _ in enumerate(review):\n",
    "        \n",
    "        sptoks = en_nlp(review[index].strip())\n",
    "\n",
    "        #Skip reviews that are bigger than allowed length\n",
    "        if len(sptoks) > max_sent_length - 2:\n",
    "            continue        \n",
    "        \n",
    "        #Get token ids\n",
    "        idx = []\n",
    "        mask = []\n",
    "        len_cnt = 0\n",
    "        for sptok in sptoks:\n",
    "            tk = sptok.text.lower()\n",
    "            \n",
    "            if len_cnt < max_sent_length:\n",
    "                idx.append(source_word2idx[tk])\n",
    "                mask.append(1.)\n",
    "                len_cnt += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        source_data.append(idx + [0] * (max_sent_length - len(idx)))\n",
    "\n",
    "        #Get relative position information\n",
    "        if mode == 'ASC':\n",
    "            pos_info = get_position_with_weights_on_polarized_words(sptoks, position[index].strip())\n",
    "            \n",
    "            if pos_info == None:\n",
    "                continue\n",
    "            \n",
    "        elif mode == 'DSC':\n",
    "            pos_info = get_position(sptoks, '0,0')\n",
    "            \n",
    "        src_loc = pos_info + [0] * (max_sent_length - len(idx))    \n",
    "            \n",
    "        source_loc.append(src_loc)\n",
    "\n",
    "        #Aspect data and masking\n",
    "        if mode == 'ASC':\n",
    "            t_sptoks = en_nlp(term[index].strip())\n",
    "            tar_idx = []\n",
    "            tar_mask = []\n",
    "            \n",
    "            for t_sptok in t_sptoks:\n",
    "                tar_idx.append(source_word2idx[tk])\n",
    "                tar_mask.append(1.)\n",
    "\n",
    "            target_data.append(tar_idx + [0] * (target_maxlen - len(tar_idx)))\n",
    "            target_mask.append(tar_mask + [0.] * (target_maxlen - len(tar_idx)))\n",
    "            target_mode.append([1., 0.])\n",
    "        \n",
    "        #Document level data and masking\n",
    "        elif mode == 'DSC':\n",
    "            target_data.append([0] * target_maxlen)\n",
    "            target_mask.append([1.] * target_maxlen)\n",
    "            target_mode.append([0., 1.])\n",
    "\n",
    "        #Get sentiment vector\n",
    "        senti = get_data_label(label[index].strip())\n",
    "        target_label.append(senti)\n",
    "\n",
    "    return np.array(source_data), \\\n",
    "           np.array(target_data), \\\n",
    "           np.array(target_label), \\\n",
    "           np.array(target_mask), \\\n",
    "           np.array(source_loc), \\\n",
    "           np.array(target_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init word embeddings from Glove (vector represntation of words)\n",
    "def init_word_embeddings(path, word2idx, DSC):\n",
    "    print('path', path)\n",
    "    wt = np.random.normal(0, 0.05, [len(word2idx), 300])\n",
    "    with open('../data/glove.840B.300d.txt', 'r',encoding= 'utf-8') as f:\n",
    "        for line in f:\n",
    "            content = line.strip().split()\n",
    "            if content[0] in word2idx:\n",
    "                if is_number(content[1]) == False: continue\n",
    "                wt[word2idx[content[0]]] = np.array(list(map(np.float32, content[1:])))\n",
    "    wt = np.asarray(wt, dtype=np.float32)\n",
    "    wt[0,:] = 0.0\n",
    "    np.save(path + DSC + '_word_embedding.npy', wt)\n",
    "    return wt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def separate_hinge_loss(prediction, label, class_num, mode, gamma):\n",
    "    '''\n",
    "    negative -0\n",
    "    neutral  -1\n",
    "    positive -2\n",
    "    m_plus = 0.9\n",
    "    m_minus = 0.1\n",
    "    lambda_val = 0.5\n",
    "    '''\n",
    "    loss = 0.0\n",
    "    for category in range(class_num):\n",
    "        if category == 0: #negative\n",
    "            m_plus = 0.9\n",
    "            m_minus = 0.1\n",
    "            lambda_val = 0.5\n",
    "        elif category == 1: #neutral\n",
    "            m_plus = 0.9\n",
    "            m_minus = 0.1\n",
    "            lambda_val = 0.5\n",
    "        elif category == 2: #positive\n",
    "            m_plus = 0.9\n",
    "            m_minus = 0.1\n",
    "            lambda_val = 0.5\n",
    "\n",
    "        vector = prediction[:,category]\n",
    "        T_c = label[:,category]\n",
    "\n",
    "        max_l = tf.square(tf.maximum(0., m_plus - vector))\n",
    "        max_r = tf.square(tf.maximum(0., vector - m_minus))\n",
    "\n",
    "        origin_L_c = T_c * max_l + lambda_val * (1 - T_c) * max_r #[batch]\n",
    "        scale_L_c = origin_L_c * gamma\n",
    "\n",
    "        L_c_concat = tf.concat([tf.expand_dims(origin_L_c,-1), tf.expand_dims(scale_L_c, -1)], -1) # [b,2]\n",
    "        L_c = tf.reduce_sum(L_c_concat * mode, -1)\n",
    "\n",
    "        margin_loss = tf.reduce_mean(L_c) #\n",
    "\n",
    "        loss += margin_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
