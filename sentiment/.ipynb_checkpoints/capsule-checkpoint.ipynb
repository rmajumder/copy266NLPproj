{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference - https://github.com/NLPWM-WHU/TransCap/tree/master/TransCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate weights and biases\n",
    "def get_weights_and_biases(w_shape,b_shape,name = None):\n",
    "    with tf.name_scope(name):\n",
    "        w_form = tf.truncated_normal(shape = w_shape,stddev= 0.1) #generate weights with standard dev 0.1\n",
    "        b_form = tf.constant(0.1,shape = b_shape)\n",
    "        return [tf.Variable(w_form),tf.Variable(b_form)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-linearity applied to vector (capsule)\n",
    "#Reference - https://arxiv.org/pdf/1710.09829.pdf\n",
    "def squash(vector):\n",
    "    epsilon = 1e-9\n",
    "    vec_squared_norm = tf.reduce_sum(tf.square(vector), -2, keepdims=True)\n",
    "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
    "    vec_squashed = scalar_factor * vector  # element-wise\n",
    "    return(vec_squashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf\n",
    "#https://arxiv.org/pdf/1703.06217.pdf \n",
    "\n",
    "def dynamic_routing(input, b_IJ, num_caps_j, len_v_j, iter_routing, incap_num, incap_dim):\n",
    "    batch_size = tf.shape(input)[0]\n",
    "    pc_num = incap_num #16\n",
    "    pc_dim = incap_dim #10\n",
    "    sc_num = num_caps_j\n",
    "    sc_dim = len_v_j\n",
    "\n",
    "    W = tf.get_variable('RoutingWeight', shape=(1, pc_num, sc_num, pc_dim, sc_dim), dtype=tf.float32,\n",
    "                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "    biases = tf.get_variable('bias', shape=(1, 1, num_caps_j, len_v_j, 1))\n",
    "\n",
    "    input = tf.tile(input, [1, 1, sc_num, 1, 1])\n",
    "    W = tf.tile(W, [batch_size, 1, 1, 1, 1])\n",
    "\n",
    "    u_hat = tf.matmul(W, input, transpose_a=True) # [batch_size, pc_num, sc_num, sc_dim, 1]\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop-gradient')\n",
    "\n",
    "    for r_iter in range(iter_routing):\n",
    "        with tf.variable_scope('iter_' + str(r_iter)):\n",
    "            c_IJ = tf.nn.softmax(b_IJ, axis=2)\n",
    "            if r_iter == iter_routing -1:\n",
    "                s_J = tf.multiply(c_IJ, u_hat)\n",
    "                s_J = tf.reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
    "                v_J = squash(s_J)\n",
    "\n",
    "            elif r_iter < iter_routing - 1:\n",
    "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
    "                s_J = tf.reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
    "                v_J = squash(s_J)\n",
    "\n",
    "                v_J_tiled = tf.tile(v_J, [1, pc_num, 1, 1, 1])\n",
    "                u_produce_v = tf.reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
    "                b_IJ += u_produce_v\n",
    "\n",
    "    return(v_J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference - https://arxiv.org/pdf/1710.09829.pdf\n",
    "\n",
    "class CapsLayer(object):\n",
    "    def __init__(self, aspect, batch_size, num_outputs, vec_len, iter_routing, with_routing=True, layer_type='FC'):\n",
    "        self.num_outputs = num_outputs\n",
    "        self.vec_len = vec_len\n",
    "        self.with_routing = with_routing\n",
    "        self.layer_type = layer_type\n",
    "        self.batch_size = batch_size\n",
    "        self.iter_routing = iter_routing\n",
    "        self.aspect = aspect\n",
    "\n",
    "    def __call__(self, input, mode=None, kernel_size=None, stride=None, embedding_dim=None):\n",
    "        if self.layer_type == 'CONV':\n",
    "            self.kernel_size = kernel_size\n",
    "            self.stride = stride\n",
    "\n",
    "            if not self.with_routing:\n",
    "                '''\n",
    "                Feature Capsules\n",
    "                '''\n",
    "                w, b = get_weights_and_biases([self.kernel_size, embedding_dim, 1, self.num_outputs * self.vec_len],\n",
    "                                                [self.num_outputs * self.vec_len], 'pc1{}'.format(self.kernel_size))\n",
    "                input_len = input.shape[1].value\n",
    "                capsule_len = input_len - self.kernel_size + 1#76\n",
    "                context_conv = tf.nn.conv2d(\n",
    "                    input=tf.reshape(input, [self.batch_size, input_len, embedding_dim, 1]),\n",
    "                    filter=w,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"capsules_context{}\".format(self.kernel_size)) # [b, 78, 1, 30*16]\n",
    "                context_conv = tf.nn.bias_add(context_conv,b)\n",
    "                '''\n",
    "                Semantic Capsules\n",
    "                '''\n",
    "                w_asp, b_asp = get_weights_and_biases([self.kernel_size, embedding_dim, 1, self.num_outputs],\n",
    "                                                [self.num_outputs], 'pc2{}'.format(self.kernel_size))\n",
    "                #aspect_info = tf.contrib.layers.fully_connected(self.aspect, 1, weights_initializer=\n",
    "                #                tf.random_uniform_initializer(minval=-0.01, maxval=0.01, seed=0.05), \n",
    "                #                                                activation_fn=None)\n",
    "\n",
    "                aspect_info = tf.contrib.layers.fully_connected(self.aspect, 1, weights_initializer=\n",
    "                                                                tf.contrib.layers.xavier_initializer(), \n",
    "                                                                activation_fn=None)\n",
    "                \n",
    "                aspect_conv = tf.nn.conv2d(\n",
    "                    input=tf.reshape(input, [self.batch_size, input_len, embedding_dim, 1]),\n",
    "                    filter=w_asp,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"capsules_aspect{}\".format(self.kernel_size)) # [b, 78, 1, 30]\n",
    "\n",
    "                aspect_gate = tf.nn.sigmoid(aspect_conv + tf.tile(tf.expand_dims(aspect_info,1),[1,capsule_len,1,self.num_outputs]))\n",
    "                aspect_gate = tf.reshape(tf.tile(tf.expand_dims(aspect_gate, -1), [1,1,1,1,self.vec_len]), [self.batch_size, -1, 1, self.num_outputs * self.vec_len])\n",
    "\n",
    "                # Aspect Routing\n",
    "                capsules_ASC = tf.expand_dims(context_conv * aspect_gate, -1)\n",
    "                capsules_DSC = tf.expand_dims(context_conv, -1)\n",
    "                capsules_concat = tf.concat([capsules_ASC, capsules_DSC], -1) # b, 78, 1, 256, 2\n",
    "                mode = tf.tile(tf.reshape(mode, [-1, 1, 1, 1, 2]), [1, capsules_concat.shape[1].value, 1, capsules_concat.shape[3].value, 1]) # b, 78, 1, 256, 2\n",
    "                capsules = tf.reduce_sum(capsules_concat * mode, -1)\n",
    "                \n",
    "                #mode = tf.tile(tf.reshape(mode, [-1, 1, 1, 1, 2]), [1, capsules_ASC.shape[1].value, 1, capsules_ASC.shape[3].value, 1]) # b, 78, 1, 256, 2\n",
    "                #capsules = tf.reduce_sum(capsules_ASC * mode, -1)\n",
    "\n",
    "                # element-wise maximum\n",
    "                capsules = tf.transpose(tf.reduce_max(tf.transpose(capsules, [0, 3, 2, 1]), -1, keepdims=True), [0, 3, 2, 1])\n",
    "                capsules = tf.reshape(capsules, (self.batch_size, self.num_outputs, self.vec_len, 1))  # [b, 16, 16, 1]\n",
    "\n",
    "                capsules = squash(capsules)\n",
    "                return (capsules)\n",
    "\n",
    "        if self.layer_type == 'FC':\n",
    "            if self.with_routing:\n",
    "                '''\n",
    "                Class Capsules\n",
    "                '''\n",
    "                incap_num = input.shape[1].value\n",
    "                incap_dim = input.shape[-2].value\n",
    "                self.input = tf.reshape(input, shape=(self.batch_size, -1, 1, input.shape[-2].value, 1))\n",
    "                with tf.variable_scope('routing'):\n",
    "                    b_IJ = tf.constant(np.zeros([1, incap_num, self.num_outputs, 1, 1], dtype=np.float32))\n",
    "                    capsules = dynamic_routing(self.input, b_IJ, self.num_outputs, self.vec_len, self.iter_routing, incap_num, incap_dim)\n",
    "                    capsules = tf.squeeze(squash(capsules), axis=1)\n",
    "            return(capsules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
