{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/NLPWM-WHU/TransCap/tree/master/TransCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from capsule import *\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MODEL(object):\n",
    "\n",
    "    def __init__(self, config, word_embedding, word_dict, data_path):\n",
    "        with tf.name_scope('parameters'):\n",
    "            self.ASC = config.ASC\n",
    "            self.DSC = config.DSC\n",
    "            self.batch_size = config.batch_size\n",
    "            self.learning_rate = config.learning_rate\n",
    "            self.n_iter = config.n_iter\n",
    "            self.gamma = config.gamma\n",
    "            self.embedding_dim = config.embedding_dim\n",
    "            self.position_dim = config.position_dim\n",
    "            self.max_sentence_len = config.max_sentence_len\n",
    "            self.max_target_len = config.max_target_len\n",
    "            self.kp1 = config.keep_prob1\n",
    "            self.kp2 = config.keep_prob2\n",
    "            self.filter_size = config.filter_size\n",
    "            self.sc_num = config.sc_num\n",
    "            self.sc_dim = config.sc_dim\n",
    "            self.cc_num = config.cc_num\n",
    "            self.cc_dim = config.cc_dim\n",
    "            self.iter_routing = config.iter_routing\n",
    "            self.w2v = word_embedding\n",
    "            self.word_id_mapping = word_dict\n",
    "            self.data_path = data_path\n",
    "\n",
    "        #Set aspect words position embedding layer\n",
    "        with tf.name_scope('embeddings'):\n",
    "            self.word_embedding = tf.Variable(self.w2v, dtype=tf.float32, name='word_embedding', trainable=False)\n",
    "            position_val = tf.Variable(tf.random_uniform(shape=[self.max_sentence_len-1, self.position_dim],\n",
    "                                       minval=-0.01, maxval=0.01, seed=0.05), dtype=tf.float32, trainable=True)\n",
    "            position_pad = tf.zeros([1, self.position_dim])\n",
    "            self.position_embedding = tf.concat([position_pad, position_val], 0)\n",
    "\n",
    "        #Initialize training variables\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.x = tf.placeholder(tf.int32, [None, self.max_sentence_len], name='x')\n",
    "            self.loc = tf.placeholder(tf.int32, [None, self.max_sentence_len], name='loc')\n",
    "            self.y = tf.placeholder(tf.int32, [None, self.cc_num], name='y')\n",
    "            self.aspect_id = tf.placeholder(tf.int32, [None,None], name='aspect_id')\n",
    "            self.tar_mask = tf.placeholder(tf.float32, [None, None], name='tar_len')\n",
    "            self.keep_prob1 = tf.placeholder(tf.float32)\n",
    "            self.keep_prob2 = tf.placeholder(tf.float32)\n",
    "            self.mode = tf.placeholder(tf.float32, [None, 2], name='mode')\n",
    "\n",
    "    def TransCap(self, inputs, target):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob1)\n",
    "        position = tf.nn.embedding_lookup(self.position_embedding, self.loc)\n",
    "        inputs = tf.concat([inputs, position], -1)\n",
    "        x_embedding = tf.expand_dims(inputs,-1)\n",
    "\n",
    "        #Capsule network (size - 16) to train on two capsule layers - features and semantic with CNN\n",
    "        with tf.variable_scope('FeatCap_SemanCap'):\n",
    "            SemanCap = CapsLayer(aspect=target, batch_size=batch_size, num_outputs=self.sc_num, vec_len=self.sc_dim,\n",
    "                                 iter_routing=self.iter_routing, with_routing=False, layer_type='CONV')\n",
    "            caps1 = SemanCap(input=x_embedding, mode=self.mode, kernel_size=self.filter_size, stride=1,\n",
    "                             embedding_dim=self.embedding_dim+self.position_dim)\n",
    "\n",
    "        #Use learned features from CNN capsule network output and \n",
    "        #train with another capsule network (size - 16) for aspect level sentiment classification \n",
    "        #with fully connected network\n",
    "        with tf.variable_scope('ASC_ClassCap'):\n",
    "            ASC_ClassCap = CapsLayer(aspect=target, batch_size=batch_size, num_outputs=self.cc_num, vec_len=self.cc_dim,\n",
    "                                     iter_routing=3, with_routing=True, layer_type='FC')\n",
    "            ASC_caps2 = ASC_ClassCap(caps1)\n",
    "            ASC_sv_length = tf.sqrt(tf.reduce_sum(tf.square(ASC_caps2), axis=2, keepdims=True) + 1e-9)\n",
    "            ASC_sprob = tf.reshape(ASC_sv_length, [batch_size, self.cc_num])\n",
    "\n",
    "        #Use learned features from CNN capsule network output and \n",
    "        #train with another capsule network (size - 16) for document level sentiment classification \n",
    "        #with fully connected network\n",
    "            \n",
    "        with tf.variable_scope('DSC_ClassCap'):\n",
    "            DSC_ClassCap = CapsLayer(aspect=target, batch_size=batch_size, num_outputs=self.cc_num, vec_len=self.cc_dim,\n",
    "                                     iter_routing=3, with_routing=True, layer_type='FC')\n",
    "            DSC_caps2 = DSC_ClassCap(caps1)\n",
    "            DSC_sv_length = tf.sqrt(tf.reduce_sum(tf.square(DSC_caps2), axis=2, keepdims=True) + 1e-9)\n",
    "            DSC_sprob = tf.reshape(DSC_sv_length, [batch_size, self.cc_num])\n",
    "\n",
    "        #Use both document \n",
    "        sprob = tf.concat([tf.expand_dims(ASC_sprob, 1), tf.expand_dims(DSC_sprob, 1)], axis=1)\n",
    "\n",
    "        return sprob\n",
    "\n",
    "    def run(self):\n",
    "        batch_size = tf.shape(self.x)[0]\n",
    "        inputs = tf.nn.embedding_lookup(self.word_embedding, self.x)\n",
    "        term = tf.nn.embedding_lookup(self.word_embedding, self.aspect_id)\n",
    "        tweight = self.tar_mask / tf.reduce_sum(self.tar_mask, 1, keepdims=True)\n",
    "        term *= tf.expand_dims(tweight, -1)\n",
    "        term = tf.reduce_sum(term, axis=1, keepdims=True)\n",
    "\n",
    "        noaspect = tf.zeros([batch_size,1,self.embedding_dim])\n",
    "        aspect_all = tf.concat([term, noaspect], axis=1)  # [b,2,300]\n",
    "        aspect = tf.matmul(tf.expand_dims(self.mode, 1), aspect_all)  # [b,1,300]\n",
    "\n",
    "        sprob = self.TransCap(inputs, aspect)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            mix_prob = tf.squeeze(tf.matmul(tf.expand_dims(self.mode,1), sprob), 1)\n",
    "            cost = separate_hinge_loss(label=tf.cast(self.y, tf.float32), prediction=mix_prob, class_num=self.cc_num, mode=self.mode, gamma=self.gamma)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step = tf.Variable(0, name=\"tr_global_step\", trainable=False)\n",
    "            # optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost, global_step=global_step)\n",
    "\n",
    "            learning_rate = tf.train.exponential_decay(self.learning_rate, global_step, decay_steps=276,\n",
    "                                                       decay_rate=0.9, staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, global_step=global_step)\n",
    "\n",
    "        with tf.name_scope('predict'):\n",
    "            true_y = tf.argmax(self.y, 1)\n",
    "            pred_y = tf.argmax(mix_prob, 1)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            # Balancing training data is helpful for CapsNet. Refer to data/{ASC}/balance.py.\n",
    "            asc_x, asc_target_word, asc_y, asc_tarmask, asc_loc, asc_mode = \\\n",
    "                read_data('{}train/balanced_'.format(self.data_path), self.word_id_mapping, \\\n",
    "                          self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "\n",
    "            dev_x, dev_target_word, dev_y, dev_tarmask, dev_loc, dev_mode = \\\n",
    "                read_data('{}dev/'.format(self.data_path), self.word_id_mapping, \\\n",
    "                          self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "\n",
    "            te_x, te_target_word, te_y, te_tarmask, te_loc, te_mode = \\\n",
    "                read_data('{}test/'.format(self.data_path), self.word_id_mapping, \\\n",
    "                          self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #==========================================\n",
    "            \n",
    "            cand_12_o_ste_x, cand_12_o_ste_target_word, cand_12_o_ste_y, cand_12_o_ste_tarmask, \\\n",
    "            cand_12_o_ste_loc, cand_12_o_ste_mode = \\\n",
    "                read_data('{}twt/candidate/2012/obama/'.format(self.data_path), self.word_id_mapping, \\\n",
    "                          self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #cand_12_r_ste_x, cand_12_r_ste_target_word, cand_12_r_ste_y, cand_12_r_ste_tarmask, \\\n",
    "            #cand_12_r_ste_loc, cand_12_r_ste_mode = \\\n",
    "            #    read_data('{}twt/candidate/2012/romney/'.format(self.data_path), self.word_id_mapping, \\\n",
    "            #              self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #cand_16_t_ste_x, cand_16_t_ste_target_word, cand_16_t_ste_y, cand_16_t_ste_tarmask, \\\n",
    "            #cand_16_t_ste_loc, cand_16_t_ste_mode = \\\n",
    "            #    read_data('{}twt/candidate/2016/trump/'.format(self.data_path), self.word_id_mapping, \\\n",
    "            #              self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #cand_16_h_ste_x, cand_16_h_ste_target_word, cand_16_h_ste_y, cand_16_h_ste_tarmask, \\\n",
    "            #cand_16_h_ste_loc, cand_16_h_ste_mode = \\\n",
    "            #    read_data('{}twt/candidate/2016/hillary/'.format(self.data_path), self.word_id_mapping, \\\n",
    "            #              self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #cand_20_t_ste_x, cand_20_t_ste_target_word, cand_20_t_ste_y, cand_20_t_ste_tarmask, \\\n",
    "            #cand_20_t_ste_loc, cand_20_t_ste_mode = \\\n",
    "            #    read_data('{}twt/candidate/2020/trump/'.format(self.data_path), self.word_id_mapping, \\\n",
    "            #              self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #cand_20_b_ste_x, cand_20_b_ste_target_word, cand_20_b_ste_y, cand_20_b_ste_tarmask, \\\n",
    "            #cand_20_b_ste_loc, cand_20_b_ste_mode = \\\n",
    "            #    read_data('{}twt/candidate/2020/biden/'.format(self.data_path), self.word_id_mapping, \\\n",
    "            #              self.max_sentence_len, self.max_target_len, 'ASC')\n",
    "            \n",
    "            #------------------------------\n",
    "\n",
    "            \n",
    "            \n",
    "            #==========================================\n",
    "\n",
    "            dsc_x, dsc_target_word, dsc_y, dsc_tarmask, dsc_loc, dsc_mode = \\\n",
    "                read_data('{}train/{}_'.format(self.data_path, self.DSC), self.word_id_mapping, \\\n",
    "                          self.max_sentence_len, self.max_target_len, 'DSC')\n",
    "\n",
    "            max_dev_acc = 0.0\n",
    "            min_dev_loss = 1000.0\n",
    "            early_stop = 0\n",
    "            max_step = 0\n",
    "            dev_acc_list = []\n",
    "            dev_loss_list = []\n",
    "            test_acc_list = []\n",
    "            test_f1_list = []\n",
    "            stest_acc_list = []\n",
    "            stest_f1_list = []\n",
    "            max_preds = []\n",
    "            dev_all_preds = []\n",
    "            \n",
    "            #======================\n",
    "            cand_12_o = []\n",
    "            cand_12_r = []\n",
    "            cand_16_t = []\n",
    "            cand_16_h = []\n",
    "            cand_20_t = []\n",
    "            cand_20_b = []\n",
    "            \n",
    "            econ_12_o = []\n",
    "            econ_12_r = []\n",
    "            econ_16_t = []\n",
    "            econ_16_h = []\n",
    "            econ_20_t = []\n",
    "            econ_20_b = []\n",
    "            \n",
    "            immg_12_o = []\n",
    "            immg_12_r = []\n",
    "            immg_16_t = []\n",
    "            immg_16_h = []\n",
    "            immg_20_t = []\n",
    "            immg_20_b = []\n",
    "            \n",
    "            hlth_12_o = []\n",
    "            hlth_12_r = []\n",
    "            hlth_16_t = []\n",
    "            hlth_16_h = []\n",
    "            hlth_20_t = []\n",
    "            hlth_20_b = []\n",
    "            \n",
    "            envr_12_o = []\n",
    "            envr_12_r = []\n",
    "            envr_16_t = []\n",
    "            envr_16_h = []\n",
    "            envr_20_t = []\n",
    "            envr_20_b = []\n",
    "            #======================\n",
    "            \n",
    "            \n",
    "            for i in range(self.n_iter):\n",
    "                '''\n",
    "                Train\n",
    "                '''\n",
    "                tr_x = np.concatenate([asc_x, dsc_x], axis=0)\n",
    "                tr_target_word = np.concatenate([asc_target_word, dsc_target_word], axis=0)\n",
    "                tr_y = np.concatenate([asc_y, dsc_y], axis=0)\n",
    "                tr_tarmask = np.concatenate([asc_tarmask, dsc_tarmask], axis=0)\n",
    "                tr_loc = np.concatenate([asc_loc, dsc_loc], axis=0)\n",
    "                tr_mode = np.concatenate([asc_mode, dsc_mode], axis=0)\n",
    "                \n",
    "                #tr_x = np.concatenate([asc_x], axis=0)\n",
    "                #tr_target_word = np.concatenate([asc_target_word], axis=0)\n",
    "                #tr_y = np.concatenate([asc_y], axis=0)\n",
    "                #tr_tarmask = np.concatenate([asc_tarmask], axis=0)\n",
    "                #tr_loc = np.concatenate([asc_loc], axis=0)\n",
    "                #tr_mode = np.concatenate([asc_mode], axis=0)\n",
    "\n",
    "                tr_loss = 0.\n",
    "                for train in self.get_batch_data(tr_x, tr_y, tr_target_word, tr_tarmask, tr_loc, tr_mode,\n",
    "                                                 self.batch_size, self.kp1, self.kp2, True):\n",
    "                                                       \n",
    "                    tr_eloss, _, step = sess.run([cost, optimizer, global_step], feed_dict=train)\n",
    "                    tr_loss += tr_eloss\n",
    "                '''\n",
    "                Test\n",
    "                '''\n",
    "                all_preds, all_labels = [], []\n",
    "                for test in self.get_batch_data(te_x, te_y, te_target_word, te_tarmask, te_loc, te_mode,\n",
    "                                                50, 1.0, 1.0, False):\n",
    "\n",
    "                    _step, ty, py, category, context = sess.run([global_step, true_y, pred_y, self.aspect_id, self.x],\n",
    "                                                                feed_dict=test)\n",
    "                    all_preds.extend(py)\n",
    "                    all_labels.extend(ty)\n",
    "                   \n",
    "                # metrics\n",
    "                \n",
    "                precision, recall, f1, support = metrics.precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "                acc = metrics.accuracy_score(all_labels, all_preds)\n",
    "                test_acc_list.append(acc)\n",
    "                test_f1_list.append(f1)\n",
    "                \n",
    "                '''\n",
    "                Small Test\n",
    "                '''\n",
    "                \n",
    "                #==========================================\n",
    "                \n",
    "                cand_12_o_sall_preds = []\n",
    "                for test in self.get_batch_data(cand_12_o_ste_x, cand_12_o_ste_y, \n",
    "                                                cand_12_o_ste_target_word, cand_12_o_ste_tarmask, \n",
    "                                                cand_12_o_ste_loc, cand_12_o_ste_mode,\n",
    "                                                50, 1.0, 1.0, False):\n",
    "\n",
    "                    step, sty, spy, category, context = sess.run(\n",
    "                        [global_step, true_y, pred_y, self.aspect_id, self.x], feed_dict=test)\n",
    "                    cand_12_o_sall_preds.extend(spy)\n",
    "                    \n",
    "                cand_12_o = cand_12_o_sall_preds\n",
    "                \n",
    "                \n",
    "                \n",
    "                #cand_12_r_sall_preds = []\n",
    "                #for test in self.get_batch_data(cand_12_r_ste_x, cand_12_r_ste_y, cand_12_r_ste_target_word, \n",
    "                #                                cand_12_r_ste_tarmask, cand_12_r_ste_loc, cand_12_r_ste_mode,\n",
    "                #                                50, 1.0, 1.0, False):\n",
    "\n",
    "                #    step, sty, spy, category, context = sess.run(\n",
    "                #        [global_step, true_y, pred_y, self.aspect_id, self.x], feed_dict=test)\n",
    "                #    cand_12_o_sall_preds.extend(spy)\n",
    "                    \n",
    "                #cand_12_r = cand_12_r_sall_preds\n",
    "                \n",
    "                #cand_16_t_sall_preds = []\n",
    "                #for test in self.get_batch_data(cand_16_t_ste_x, cand_16_t_ste_y, cand_16_t_ste_target_word, \n",
    "                #                                cand_16_t_ste_tarmask, cand_16_t_ste_loc, cand_16_t_ste_mode,\n",
    "                #                                50, 1.0, 1.0, False):\n",
    "\n",
    "                #    step, sty, spy, category, context = sess.run(\n",
    "                #        [global_step, true_y, pred_y, self.aspect_id, self.x], feed_dict=test)\n",
    "                #    cand_16_t_sall_preds.extend(spy)\n",
    "                    \n",
    "                #cand_16_t = cand_16_t_sall_preds\n",
    "                \n",
    "                #cand_16_h_sall_preds = []\n",
    "                #for test in self.get_batch_data(cand_16_h_ste_x, cand_16_h_ste_y, cand_16_h_ste_target_word, \n",
    "                #                                cand_16_h_ste_tarmask, cand_16_h_ste_loc, cand_16_h_ste_mode,\n",
    "                #                                50, 1.0, 1.0, False):\n",
    "\n",
    "                #    step, sty, spy, category, context = sess.run(\n",
    "                #        [global_step, true_y, pred_y, self.aspect_id, self.x], feed_dict=test)\n",
    "                #    cand_16_h_sall_preds.extend(spy)\n",
    "                    \n",
    "                #cand_16_h = cand_16_h_sall_preds\n",
    "                \n",
    "                #cand_20_t_sall_preds = []\n",
    "                #for test in self.get_batch_data(cand_20_t_ste_x, cand_20_t_ste_y, cand_20_t_ste_target_word, \n",
    "                #                                cand_20_t_ste_tarmask, cand_20_t_ste_loc, cand_20_t_ste_mode,\n",
    "                #                                50, 1.0, 1.0, False):\n",
    "\n",
    "                #    step, sty, spy, category, context = sess.run(\n",
    "                #        [global_step, true_y, pred_y, self.aspect_id, self.x], feed_dict=test)\n",
    "                #    cand_20_t_sall_preds.extend(spy)\n",
    "                    \n",
    "                #cand_20_t = cand_20_t_sall_preds\n",
    "                \n",
    "                #cand_20_b_sall_preds = []\n",
    "                #for test in self.get_batch_data(cand_20_b_ste_x, cand_20_b_ste_y, cand_20_b_ste_target_word, \n",
    "                #                                cand_20_b_ste_tarmask, cand_20_b_ste_loc, cand_20_b_ste_mode,\n",
    "                #                                50, 1.0, 1.0, False):\n",
    "\n",
    "                #    step, sty, spy, category, context = sess.run(\n",
    "                #        [global_step, true_y, pred_y, self.aspect_id, self.x], feed_dict=test)\n",
    "                #    cand_20_b_sall_preds.extend(spy)\n",
    "                    \n",
    "                #cand_20_b = cand_20_b_sall_preds\n",
    "                \n",
    "                \n",
    "                #sall_labels.extend(sty)\n",
    "                \n",
    "                #sprecision, srecall, sf1, ssupport = metrics.precision_recall_fscore_support(\n",
    "                #    sall_labels, sall_preds, average='macro')\n",
    "                #sacc = metrics.accuracy_score(sall_labels, sall_preds)\n",
    "                #stest_acc_list.append(sacc)\n",
    "                #stest_f1_list.append(sf1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #==========================================\n",
    "                \n",
    "                '''\n",
    "                Dev\n",
    "                '''\n",
    "                dev_acc, dev_loss = 0., 0.\n",
    "                dev_all_preds = []\n",
    "                dev_all_labels = []\n",
    "                for dev in self.get_batch_data(dev_x, dev_y, dev_target_word, dev_tarmask, dev_loc, dev_mode,\n",
    "                                               50, 1.0, 1.0, False):\n",
    "                    dev_eloss, dev_step, dev_ty, dev_py = sess.run([cost, global_step, true_y, pred_y],\n",
    "                                                                   feed_dict=dev)\n",
    "                    dev_loss += dev_eloss\n",
    "                    dev_all_preds.extend(dev_ty)\n",
    "                    dev_all_labels.extend(dev_py)\n",
    "                dev_acc = metrics.accuracy_score(dev_all_labels, dev_all_preds)\n",
    "                dev_acc_list.append(dev_acc)\n",
    "                dev_loss_list.append(dev_loss)\n",
    "                dev_all_preds = dev_all_preds\n",
    "                \n",
    "                '''\n",
    "                Early Stopping\n",
    "                '''\n",
    "                if (dev_acc > max_dev_acc) or (dev_loss < min_dev_loss):\n",
    "                    early_stop = 0\n",
    "                    if (dev_acc > max_dev_acc): max_dev_acc = dev_acc\n",
    "                    if (dev_loss < min_dev_loss): min_dev_loss = dev_loss\n",
    "                else:\n",
    "                    early_stop += 1\n",
    "                if early_stop >= 5:\n",
    "                    break\n",
    "                if early_stop > max_step:\n",
    "                    max_step = early_stop\n",
    "\n",
    "                print('\\n{:-^80}'.format('Iter'+str(i)))\n",
    "                print('train loss={:.6f}, dev loss={:.6f}, dev acc={:.4f}, step={}'\n",
    "                      .format(tr_loss, dev_loss, dev_acc, step))\n",
    "                print('test acc={:.4f}, test precision={:.4f}, test recall={:.4f}, test f1={:.4f}'\n",
    "                      .format(acc, precision, recall, f1))\n",
    "                print('smalltest acc={:.4f}, test precision={:.4f}, test recall={:.4f}, test f1={:.4f}'\n",
    "                      .format(sacc, sprecision, srecall, sf1))\n",
    "                print('max step:{}, early stop step:{}'.format(max_step, early_stop))\n",
    "                \n",
    "                text_file = open(\"results.txt\", \"a\")\n",
    "                text_file.write(\"\\n\")\n",
    "                text_file.write('\\n{:-^80}'.format('Iter'+str(i)))\n",
    "                text_file.write(\"\\n\")\n",
    "                text_file.write('train loss={:.6f}, dev loss={:.6f}, dev acc={:.4f}, step={}'\n",
    "                      .format(tr_loss, dev_loss, dev_acc, step))\n",
    "                text_file.write(\"\\n\")\n",
    "                text_file.write('test acc={:.4f}, test precision={:.4f}, test recall={:.4f}, test f1={:.4f}'\n",
    "                      .format(acc, precision, recall, f1))\n",
    "                text_file.write(\"\\n\")\n",
    "                \n",
    "                #text_file.write('smalltest acc={:.4f}, test precision={:.4f}, test recall={:.4f}, test f1={:.4f}'\n",
    "                #      .format(sacc, sprecision, srecall, sf1))\n",
    "                #text_file.write(\"\\n\")\n",
    "                text_file.write('max step:{}, early stop step:{}'.format(max_step, early_stop))\n",
    "                text_file.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('\\n{:-^80}'.format('Mission Complete'))\n",
    "            max_acc_index = dev_acc_list.index(max(dev_acc_list))\n",
    "            print(\"max acc_index:\", max_acc_index)\n",
    "            print('test_acc: {:.4f},test_f1:{:.4f}'.format(test_acc_list[max_acc_index], test_f1_list[max_acc_index]))\n",
    "            min_loss_index = dev_loss_list.index(min(dev_loss_list))\n",
    "            print(\"min loss_index:\", min_loss_index)\n",
    "            print('test_acc: {:.4f},test_f1:{:.4f}\\n'.format(test_acc_list[min_loss_index], test_f1_list[min_loss_index]))\n",
    "\n",
    "            text_file = open(\"results.txt\", \"a\")\n",
    "            text_file.write(\"\\n\")\n",
    "            text_file.write('\\n{:-^80}'.format('Mission Complete'))\n",
    "            text_file.write(\"\\n\")\n",
    "            text_file.write(\"max acc_index:\" + str(max_acc_index))\n",
    "            text_file.write(\"\\n\")\n",
    "            text_file.write('test_acc: {:.4f},test_f1:{:.4f}'.format(test_acc_list[max_acc_index], test_f1_list[max_acc_index]))\n",
    "            min_loss_index = dev_loss_list.index(min(dev_loss_list))\n",
    "            text_file.write(\"\\n\")\n",
    "            text_file.write(\"min loss_index:\" + str(min_loss_index))\n",
    "            text_file.write(\"\\n\")\n",
    "            text_file.write('test_acc: {:.4f},test_f1:{:.4f}\\n'.format(test_acc_list[min_loss_index], test_f1_list[min_loss_index]))\n",
    "            text_file.close()\n",
    "            \n",
    "            #=======================\n",
    "            \n",
    "            t_sentiment = open(\"sentresult/cand_12_o.txt\", \"a\")\n",
    "            for mp in cand_12_o:\n",
    "                t_sentiment.write(str(mp))\n",
    "                t_sentiment.write(\"\\n\")\n",
    "            t_sentiment.close()\n",
    "            \n",
    "            t_sentiment = open(\"sentresult/cand_12_r.txt\", \"a\")\n",
    "            for mp in cand_12_r:\n",
    "                t_sentiment.write(str(mp))\n",
    "                t_sentiment.write(\"\\n\")\n",
    "            t_sentiment.close()\n",
    "            \n",
    "            t_sentiment = open(\"sentresult/cand_16_t.txt\", \"a\")\n",
    "            for mp in cand_16_t:\n",
    "                t_sentiment.write(str(mp))\n",
    "                t_sentiment.write(\"\\n\")\n",
    "            t_sentiment.close()\n",
    "            \n",
    "            t_sentiment = open(\"sentresult/cand_16_h.txt\", \"a\")\n",
    "            for mp in cand_16_h:\n",
    "                t_sentiment.write(str(mp))\n",
    "                t_sentiment.write(\"\\n\")\n",
    "            t_sentiment.close()\n",
    "            \n",
    "            t_sentiment = open(\"sentresult/cand_20_t.txt\", \"a\")\n",
    "            for mp in cand_20_t:\n",
    "                t_sentiment.write(str(mp))\n",
    "                t_sentiment.write(\"\\n\")\n",
    "            t_sentiment.close()\n",
    "            \n",
    "            t_sentiment = open(\"sentresult/cand_20_b.txt\", \"a\")\n",
    "            for mp in cand_20_b:\n",
    "                t_sentiment.write(str(mp))\n",
    "                t_sentiment.write(\"\\n\")\n",
    "            t_sentiment.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #=======================\n",
    "            \n",
    "    def get_batch_data(self, x, y, target_words, tar_mask, loc, mode, batch_size, keep_prob1, keep_prob2, is_shuffle=True):\n",
    "        length = len(y)\n",
    "        all_index = np.arange(length)\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(all_index)\n",
    "         \n",
    "    \n",
    "        for i in range(int(length / batch_size) + (1 if length % batch_size else 0)):\n",
    "            index = all_index[i * batch_size:(i + 1) * batch_size]\n",
    "            \n",
    "            feed_dict = {}\n",
    "            \n",
    "            try:\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.x: x[index],\n",
    "                    self.y: y[index],\n",
    "                    self.loc: loc[index],\n",
    "                    self.aspect_id: target_words[index],\n",
    "                    self.tar_mask: tar_mask[index],\n",
    "                    self.mode: mode[index],\n",
    "                    self.keep_prob1: np.array([keep_prob1]),\n",
    "                    self.keep_prob2: np.array([keep_prob2])\n",
    "                }\n",
    "            except:\n",
    "                print('Exception with feed dict')\n",
    "                print(loc[index])\n",
    "                continue\n",
    "                        \n",
    "            yield feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
